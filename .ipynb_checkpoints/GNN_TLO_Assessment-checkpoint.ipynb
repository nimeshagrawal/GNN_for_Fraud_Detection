{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4TTOloSWm15z"
   },
   "source": [
    "<h1>GNNs for Fraud Detection </h1>\n",
    "This assessment will be divided into 2 parts:\n",
    "\n",
    "- [Week 1] During the first part, you'll discover how to import a CSV file and create a graph dataset. You'll have fun experimenting with functions and learning to sample batches of sub-graphs for training. Make sure to complete this part before the end of week 1. There are 3 sections in this part, each section is worth 5 points.\n",
    "\n",
    "- [Week 2] In the second part, you'll construct your very own GCN model and train it using the data you prepared in week 1. Get ready to watch your fraud detection system come to life! There are 2 sections in this part, each section is worth 5 points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FgRCCf85oHlf",
    "outputId": "ace64ea3-73bd-46e0-99a7-f16b0a8d09b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://data.dgl.ai/wheels/repo.html\n",
      "Collecting dgl\n",
      "  Downloading https://data.dgl.ai/wheels/dgl-1.0.2-cp310-cp310-win_amd64.whl (3.0 MB)\n",
      "     ---------------------------------------- 3.0/3.0 MB 1.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\nimes\\anaconda3\\envs\\nimesh\\lib\\site-packages (from dgl) (1.8.0)\n",
      "Requirement already satisfied: networkx>=2.1 in c:\\users\\nimes\\anaconda3\\envs\\nimesh\\lib\\site-packages (from dgl) (2.7)\n",
      "Requirement already satisfied: tqdm in c:\\users\\nimes\\anaconda3\\envs\\nimesh\\lib\\site-packages (from dgl) (4.64.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\nimes\\anaconda3\\envs\\nimesh\\lib\\site-packages (from dgl) (2.28.2)\n",
      "Requirement already satisfied: psutil>=5.8.0 in c:\\users\\nimes\\anaconda3\\envs\\nimesh\\lib\\site-packages (from dgl) (5.9.0)\n",
      "Requirement already satisfied: numpy>=1.14.0 in c:\\users\\nimes\\anaconda3\\envs\\nimesh\\lib\\site-packages (from dgl) (1.23.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nimes\\anaconda3\\envs\\nimesh\\lib\\site-packages (from requests>=2.19.0->dgl) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nimes\\anaconda3\\envs\\nimesh\\lib\\site-packages (from requests>=2.19.0->dgl) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nimes\\anaconda3\\envs\\nimesh\\lib\\site-packages (from requests>=2.19.0->dgl) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\nimes\\anaconda3\\envs\\nimesh\\lib\\site-packages (from requests>=2.19.0->dgl) (1.26.14)\n",
      "Requirement already satisfied: colorama in c:\\users\\nimes\\anaconda3\\envs\\nimesh\\lib\\site-packages (from tqdm->dgl) (0.4.6)\n",
      "Installing collected packages: dgl\n",
      "Successfully installed dgl-1.0.2\n",
      "Looking in links: https://data.dgl.ai/wheels-test/repo.html\n",
      "Collecting dglgo\n",
      "  Downloading dglgo-0.0.2-py3-none-any.whl (63 kB)\n",
      "     ---------------------------------------- 63.5/63.5 kB 1.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: PyYAML>=5.1 in c:\\users\\nimes\\anaconda3\\envs\\nimesh\\lib\\site-packages (from dglgo) (6.0)\n",
      "Collecting ruamel.yaml>=0.17.20\n",
      "  Downloading ruamel.yaml-0.17.21-py3-none-any.whl (109 kB)\n",
      "     -------------------------------------- 109.5/109.5 kB 3.1 MB/s eta 0:00:00\n",
      "Collecting typer>=0.4.0\n",
      "  Downloading typer-0.7.0-py3-none-any.whl (38 kB)\n",
      "Collecting ogb>=1.3.3\n",
      "  Downloading ogb-1.3.6-py3-none-any.whl (78 kB)\n",
      "     ---------------------------------------- 78.8/78.8 kB 2.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in c:\\users\\nimes\\anaconda3\\envs\\nimesh\\lib\\site-packages (from dglgo) (1.2.0)\n",
      "Collecting rdkit-pypi\n",
      "  Downloading rdkit_pypi-2022.9.5-cp310-cp310-win_amd64.whl (20.5 MB)\n",
      "     ---------------------------------------- 20.5/20.5 MB 1.8 MB/s eta 0:00:00\n",
      "Collecting pydantic>=1.9.0\n",
      "  Downloading pydantic-1.10.7-cp310-cp310-win_amd64.whl (2.1 MB)\n",
      "     ---------------------------------------- 2.1/2.1 MB 2.2 MB/s eta 0:00:00\n",
      "Collecting autopep8>=1.6.0\n",
      "  Downloading autopep8-2.0.2-py2.py3-none-any.whl (45 kB)\n",
      "     ---------------------------------------- 45.2/45.2 kB 2.2 MB/s eta 0:00:00\n",
      "Collecting isort>=5.10.1\n",
      "  Downloading isort-5.12.0-py3-none-any.whl (91 kB)\n",
      "     ---------------------------------------- 91.2/91.2 kB 2.6 MB/s eta 0:00:00\n",
      "Collecting numpydoc>=1.1.0\n",
      "  Downloading numpydoc-1.5.0-py3-none-any.whl (52 kB)\n",
      "     ---------------------------------------- 52.4/52.4 kB ? eta 0:00:00\n",
      "Collecting pycodestyle>=2.10.0\n",
      "  Downloading pycodestyle-2.10.0-py2.py3-none-any.whl (41 kB)\n",
      "     ---------------------------------------- 41.3/41.3 kB 1.0 MB/s eta 0:00:00\n",
      "Collecting tomli\n",
      "  Downloading tomli-2.0.1-py3-none-any.whl (12 kB)\n",
      "Collecting sphinx>=4.2\n",
      "  Downloading sphinx-6.1.3-py3-none-any.whl (3.0 MB)\n",
      "     ---------------------------------------- 3.0/3.0 MB 2.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: Jinja2>=2.10 in c:\\users\\nimes\\anaconda3\\envs\\nimesh\\lib\\site-packages (from numpydoc>=1.1.0->dglgo) (3.1.2)\n",
      "Collecting outdated>=0.2.0\n",
      "  Downloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\n",
      "Requirement already satisfied: numpy>=1.16.0 in c:\\users\\nimes\\anaconda3\\envs\\nimesh\\lib\\site-packages (from ogb>=1.3.3->dglgo) (1.23.5)\n",
      "Requirement already satisfied: pandas>=0.24.0 in c:\\users\\nimes\\anaconda3\\envs\\nimesh\\lib\\site-packages (from ogb>=1.3.3->dglgo) (1.5.3)\n",
      "Requirement already satisfied: urllib3>=1.24.0 in c:\\users\\nimes\\anaconda3\\envs\\nimesh\\lib\\site-packages (from ogb>=1.3.3->dglgo) (1.26.14)\n",
      "Requirement already satisfied: tqdm>=4.29.0 in c:\\users\\nimes\\anaconda3\\envs\\nimesh\\lib\\site-packages (from ogb>=1.3.3->dglgo) (4.64.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\nimes\\anaconda3\\envs\\nimesh\\lib\\site-packages (from ogb>=1.3.3->dglgo) (1.16.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\nimes\\anaconda3\\envs\\nimesh\\lib\\site-packages (from ogb>=1.3.3->dglgo) (1.12.1)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\nimes\\anaconda3\\envs\\nimesh\\lib\\site-packages (from pydantic>=1.9.0->dglgo) (4.4.0)\n",
      "Collecting ruamel.yaml.clib>=0.2.6\n",
      "  Downloading ruamel.yaml.clib-0.2.7-cp310-cp310-win_amd64.whl (111 kB)\n",
      "     -------------------------------------- 111.7/111.7 kB 3.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\nimes\\anaconda3\\envs\\nimesh\\lib\\site-packages (from scikit-learn>=0.20.0->dglgo) (1.8.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\nimes\\anaconda3\\envs\\nimesh\\lib\\site-packages (from scikit-learn>=0.20.0->dglgo) (2.2.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\nimes\\anaconda3\\envs\\nimesh\\lib\\site-packages (from scikit-learn>=0.20.0->dglgo) (1.1.1)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\nimes\\anaconda3\\envs\\nimesh\\lib\\site-packages (from typer>=0.4.0->dglgo) (8.1.3)\n",
      "Requirement already satisfied: Pillow in c:\\users\\nimes\\anaconda3\\envs\\nimesh\\lib\\site-packages (from rdkit-pypi->dglgo) (9.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\nimes\\anaconda3\\envs\\nimesh\\lib\\site-packages (from click<9.0.0,>=7.1.1->typer>=0.4.0->dglgo) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\nimes\\anaconda3\\envs\\nimesh\\lib\\site-packages (from Jinja2>=2.10->numpydoc>=1.1.0->dglgo) (2.1.1)\n",
      "Requirement already satisfied: requests in c:\\users\\nimes\\anaconda3\\envs\\nimesh\\lib\\site-packages (from outdated>=0.2.0->ogb>=1.3.3->dglgo) (2.28.2)\n",
      "Collecting littleutils\n",
      "  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: setuptools>=44 in c:\\users\\nimes\\anaconda3\\envs\\nimesh\\lib\\site-packages (from outdated>=0.2.0->ogb>=1.3.3->dglgo) (65.6.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\nimes\\anaconda3\\envs\\nimesh\\lib\\site-packages (from pandas>=0.24.0->ogb>=1.3.3->dglgo) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\nimes\\anaconda3\\envs\\nimesh\\lib\\site-packages (from pandas>=0.24.0->ogb>=1.3.3->dglgo) (2022.7.1)\n",
      "Collecting sphinxcontrib-htmlhelp>=2.0.0\n",
      "  Downloading sphinxcontrib_htmlhelp-2.0.1-py3-none-any.whl (99 kB)\n",
      "     ---------------------------------------- 99.8/99.8 kB 2.9 MB/s eta 0:00:00\n",
      "Collecting sphinxcontrib-applehelp\n",
      "  Downloading sphinxcontrib_applehelp-1.0.4-py3-none-any.whl (120 kB)\n",
      "     -------------------------------------- 120.6/120.6 kB 6.9 MB/s eta 0:00:00\n",
      "Collecting docutils<0.20,>=0.18\n",
      "  Downloading docutils-0.19-py3-none-any.whl (570 kB)\n",
      "     -------------------------------------- 570.5/570.5 kB 2.6 MB/s eta 0:00:00\n",
      "Collecting babel>=2.9\n",
      "  Downloading Babel-2.12.1-py3-none-any.whl (10.1 MB)\n",
      "     ---------------------------------------- 10.1/10.1 MB 2.3 MB/s eta 0:00:00\n",
      "Collecting sphinxcontrib-devhelp\n",
      "  Downloading sphinxcontrib_devhelp-1.0.2-py2.py3-none-any.whl (84 kB)\n",
      "     -------------------------------------- 84.7/84.7 kB 676.8 kB/s eta 0:00:00\n",
      "Collecting snowballstemmer>=2.0\n",
      "  Downloading snowballstemmer-2.2.0-py2.py3-none-any.whl (93 kB)\n",
      "     ---------------------------------------- 93.0/93.0 kB 2.7 MB/s eta 0:00:00\n",
      "Collecting imagesize>=1.3\n",
      "  Downloading imagesize-1.4.1-py2.py3-none-any.whl (8.8 kB)\n",
      "Collecting Pygments>=2.13\n",
      "  Downloading Pygments-2.15.0-py3-none-any.whl (1.1 MB)\n",
      "     ---------------------------------------- 1.1/1.1 MB 1.9 MB/s eta 0:00:00\n",
      "Collecting sphinxcontrib-qthelp\n",
      "  Downloading sphinxcontrib_qthelp-1.0.3-py2.py3-none-any.whl (90 kB)\n",
      "     ---------------------------------------- 90.6/90.6 kB 2.6 MB/s eta 0:00:00\n",
      "Collecting sphinxcontrib-jsmath\n",
      "  Downloading sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl (5.1 kB)\n",
      "Collecting alabaster<0.8,>=0.7\n",
      "  Downloading alabaster-0.7.13-py3-none-any.whl (13 kB)\n",
      "Collecting sphinxcontrib-serializinghtml>=1.1.5\n",
      "  Downloading sphinxcontrib_serializinghtml-1.1.5-py2.py3-none-any.whl (94 kB)\n",
      "     ---------------------------------------- 94.0/94.0 kB 1.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging>=21.0 in c:\\users\\nimes\\anaconda3\\envs\\nimesh\\lib\\site-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (22.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nimes\\anaconda3\\envs\\nimesh\\lib\\site-packages (from requests->outdated>=0.2.0->ogb>=1.3.3->dglgo) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nimes\\anaconda3\\envs\\nimesh\\lib\\site-packages (from requests->outdated>=0.2.0->ogb>=1.3.3->dglgo) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nimes\\anaconda3\\envs\\nimesh\\lib\\site-packages (from requests->outdated>=0.2.0->ogb>=1.3.3->dglgo) (3.4)\n",
      "Building wheels for collected packages: littleutils\n",
      "  Building wheel for littleutils (setup.py): started\n",
      "  Building wheel for littleutils (setup.py): finished with status 'done'\n",
      "  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7028 sha256=fc811f6d551aeef4459b3ede5fb838d72562523bbdc56611b2d43f6833d9f367\n",
      "  Stored in directory: c:\\users\\nimes\\appdata\\local\\pip\\cache\\wheels\\e0\\3b\\9c\\d55ff5bc6cfbe70537c4731a22f2ee2462c2e5010b56ac9726\n",
      "Successfully built littleutils\n",
      "Installing collected packages: snowballstemmer, littleutils, tomli, sphinxcontrib-serializinghtml, sphinxcontrib-qthelp, sphinxcontrib-jsmath, sphinxcontrib-htmlhelp, sphinxcontrib-devhelp, sphinxcontrib-applehelp, ruamel.yaml.clib, rdkit-pypi, Pygments, pydantic, pycodestyle, isort, imagesize, docutils, babel, alabaster, typer, sphinx, ruamel.yaml, outdated, autopep8, ogb, numpydoc, dglgo\n",
      "  Attempting uninstall: Pygments\n",
      "    Found existing installation: Pygments 2.11.2\n",
      "    Uninstalling Pygments-2.11.2:\n",
      "      Successfully uninstalled Pygments-2.11.2\n",
      "Successfully installed Pygments-2.15.0 alabaster-0.7.13 autopep8-2.0.2 babel-2.12.1 dglgo-0.0.2 docutils-0.19 imagesize-1.4.1 isort-5.12.0 littleutils-0.2.2 numpydoc-1.5.0 ogb-1.3.6 outdated-0.2.2 pycodestyle-2.10.0 pydantic-1.10.7 rdkit-pypi-2022.9.5 ruamel.yaml-0.17.21 ruamel.yaml.clib-0.2.7 snowballstemmer-2.2.0 sphinx-6.1.3 sphinxcontrib-applehelp-1.0.4 sphinxcontrib-devhelp-1.0.2 sphinxcontrib-htmlhelp-2.0.1 sphinxcontrib-jsmath-1.0.1 sphinxcontrib-qthelp-1.0.3 sphinxcontrib-serializinghtml-1.1.5 tomli-2.0.1 typer-0.7.0\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Note: We will train our GNNs on CPU runtime since we have a very small graph and training time should be fairly low, \n",
    "you can use GPUs if you wish, but make sure that you install the right DGL version from here- https://www.dgl.ai/pages/start.html\n",
    "The below code installs DGL for a CPU runtime\n",
    "'''\n",
    "\n",
    "!pip install  dgl -f https://data.dgl.ai/wheels/repo.html\n",
    "!pip install  dglgo -f https://data.dgl.ai/wheels-test/repo.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "jFkv8IJYJdPQ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DGL backend not selected or invalid.  Assuming PyTorch for now.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n"
     ]
    }
   ],
   "source": [
    "#Don't bother if you get this warning message- \"DGL backend not selected or invalid.  Assuming PyTorch for now.\"\n",
    "import torch\n",
    "import dgl\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZXeXChpDoToG"
   },
   "source": [
    "<h2>Week 1</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B6Tu8L-5ocVV"
   },
   "source": [
    "<h3>Loading Your First Graph Dataset </h3>\n",
    "You've been provided with 2 csv files that contain an open source fraud detection dataset created by Amazon. \n",
    "\n",
    "<h4> Amazon Fraud Detection Dataset </h4>\n",
    "The Amazon dataset encompasses product evaluations in the Musical Instruments category. Individuals with over 80% helpful votes are identified as benign entities, while those with fewer than 20% helpful votes are considered fraudulent entities. Performing a fraudulent user detection task on the Amazon dataset involves a binary classification process. Each of these users have a 25-dim dense feature representation that is obtained by calculating certain statistical properties of the user's behaviors. Features include properties like entropy of user's ratings, time entropy, sentiment of user's reviews etc. You can learn more about the features from Table 1. in the paper-https://arxiv.org/pdf/2005.10150.pdf.\n",
    "\n",
    "The nodes in the graph are therefore users on the Amazon e-commerce platform, the nodes also have handcrafted-features. The node information is available in the file below\n",
    "- node_information.csv: contains node_id as the first column and features 1-25 in the corresponding columns, the last column is the label of the user (benign, fraudulent)\n",
    "\n",
    "To create a network of interconnected users and generate a graph, we link users who share similarities. The file provided contains connections between users exhibiting the top 5% mutual review text similarities (calculated using TF-IDF) among all users. In other words, users with high textual resemblances are connected, based on the assumption that this structure could reveal insights into the communication patterns among fraudulent users.\n",
    "\n",
    "- edge_data.csv: contains 2 columns with source and destination node ids indicating an edge between the source and destination columns\n",
    "\n",
    "Please implement the functions that have  \"NotImplementedError()\" marked and then run the driver-code run functions that you've implemented. In order to test individual functions, you can run the corresponding driver code\n",
    "\n",
    "NOTE: Kindly be aware that the open-source graph has been adapted to meet the requirements of this assignment. Please utilize the files provided to you and refrain from downloading files from external sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8oC997F5oscE",
    "outputId": "69469b03-98fa-49e0-9e7e-b65d2ba4359f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "#@title\n",
    "\n",
    "''' \n",
    "The dataset for this project is available in the shared directory: https://drive.google.com/drive/folders/1hiPnkO9VcQTMptBLvbC5WGfJS6gQtOaS?usp=sharing\n",
    "Please create a copy in your own Google Drive and mount the path below\n",
    "'''\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ncpaFJdfoaHB"
   },
   "outputs": [],
   "source": [
    "#import required packages\n",
    "import torch\n",
    "import dgl\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vrJK-cHGooDw"
   },
   "outputs": [],
   "source": [
    "#Section 1: Data Loading\n",
    "def load_node_information_from_csv(path: str):\n",
    "    '''Given a path to the node information csv file, create a tensor of node \n",
    "    features and corresponding labels. You can load using the Pandas library\n",
    "    Args:\n",
    "        path: path to a csv file\n",
    "    Returns: \n",
    "        a tensor of node features of the shape (num_nodes, num_features) and a tensor of \n",
    "        node labels of the shape (num_nodes)\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    node_info = pd.read_csv(path)\n",
    "    node_labels = torch.from_numpy(node_info['label'].values)\n",
    "    node_features = torch.from_numpy(node_info.drop(['label'], axis=1).values)\n",
    "#     raise NotImplementedError\n",
    "    return node_features, node_labels\n",
    "    \n",
    "\n",
    "def load_edges_from_csv(path: str):\n",
    "    '''Given a path to a csv file, create a tuple of tensors, you can use the Pandas library\n",
    "    Args:\n",
    "        path: path to a csv file\n",
    "    Returns: \n",
    "        src: a pytorch tensor of source node ids\n",
    "        dst: a pytorch tensor of destination node ids\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    # make sure that the node ids are in the required type format, ie. int64\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "\n",
    "def create_graph_from_tensors(src_tensor: torch.Tensor, dst_tensor: torch.Tensor):\n",
    "    '''Given a tuple of edge tensors (u,v), create a graph such that each element in u is \n",
    "    connected to each element in v with a one-to-one mapping\n",
    "    please refer to: https://docs.dgl.ai/en/1.0.x/generated/dgl.graph.html\n",
    "    For example: \n",
    "    u = th.tensor([1, 2, 3]), \n",
    "    v = th.tensor([4, 5, 0]) \n",
    "    should create a graph with 6 nodes and 3 edges:\n",
    "    1 -> 4, 2 -> 5, 3 -> 0\n",
    "    Args:\n",
    "        edge_tensors: a tuple of edge tensors\n",
    "    Returns: \n",
    "        a DGL graph\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "\n",
    "\n",
    "def add_node_features_and_labels(graph: dgl.DGLGraph, node_features: torch.Tensor, node_labels: torch.Tensor):\n",
    "    '''Given a graph and a tensor of node features and labels, add the node features and labels to \n",
    "    the graph object so as to access them later directly from the graph object. \n",
    "    **Name the features and labels as \"features\" and \"labels\" respectively**\n",
    "    please refer to: https://docs.dgl.ai/guide/graph-feature.html?highlight=features\n",
    "    Args:\n",
    "        graph: a DGL graph\n",
    "        node_features: a tensor of node features of type float()\n",
    "    Returns: \n",
    "        a DGL graph with node features with shape (num_nodes, num_features) and labels with shape (num_nodes, 1)\n",
    "    '''\n",
    "\n",
    "    #**Name the features and labels as \"features\" and \"labels\" respectively**\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I7Ds3OyNsFpP"
   },
   "outputs": [],
   "source": [
    "# Section 2: data exploration\n",
    "# play around with your data\n",
    "def get_num_nodes(graph: dgl.DGLGraph):\n",
    "    '''Given a DGL graph, return the number of nodes\n",
    "    please refer to: https://docs.dgl.ai/en/0.1.x/api/python/graph.html#querying-graph-structure\n",
    "    Args:\n",
    "        graph: a DGL graph\n",
    "    Returns: \n",
    "        the number of nodes in the graph\n",
    "    '''\n",
    "    # # YOUR CODE HERE\n",
    "    # raise NotImplementedError()\n",
    "    return graph.num_nodes()\n",
    "\n",
    "\n",
    "def check_if_edge_exists(graph: dgl.DGLGraph, u: int, v: int):\n",
    "    '''Given a DGL graph and two nodes u and v, \n",
    "    return True if the edge (u,v) exists in the graph, False otherwise\n",
    "    please refer to: https://docs.dgl.ai/en/0.1.x/api/python/graph.html#querying-graph-structure\n",
    "    Args:\n",
    "        graph: a DGL graph\n",
    "        u: a node\n",
    "        v: a node\n",
    "    Returns: \n",
    "        True if the edge (u,v) exists in the graph, False otherwise\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "def get_first_hop_neighbors(graph: dgl.DGLGraph, node: int):\n",
    "    '''Given a DGL graph and a node, return the first hop neighbors of the node\n",
    "       DO NOT USE DGL's built-in neighbor sampler. First hop neighbors are the nodes that are directly connected to the node\n",
    "       please refer to: https://docs.dgl.ai/en/0.1.x/api/python/graph.html#querying-graph-structure\n",
    "    Args:\n",
    "        graph: a DGL graph\n",
    "        node: a node\n",
    "    Returns: \n",
    "        a list of first hop neighbors of the node\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "def get_second_hop_neighbors(graph: dgl.DGLGraph, node: int):\n",
    "    '''Given a DGL graph and a node, return the second hop neighbors of the node\n",
    "       DO NOT USE DGL's built-in neighbor sampler. Second hop neighbors are the nodes that are connected to the first hop neighbors of the node\n",
    "    Args:\n",
    "        graph: a DGL graph\n",
    "        node: a node\n",
    "    Returns: \n",
    "        a tensor of second hop neighbors of the node\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7RSEva6yu8G1"
   },
   "source": [
    "<h4>Data Sampling</h4>\n",
    "As touched upon in the reading material, graphs possess a relational nature, distinguishing them from datasets like images or text, which maintain a fixed context window. Consequently, when sampling a node for training, it's essential to also sample the neighbors we want to include for aggregation. We'll delve deeper into the GCN model next week, but for now, remember that graph neural networks learn from both node-specific information (i.e., node features) and structural information (a node's neighborhood). As a result, data batches typically consist of a node's subgraph, including its neighborhood in a particular manner. For example, we can consider a node's first and second hop neighbors as its neighborhood. Alternatively, we could use a fixed number of neighbors (either randomly or through a ranking process) in each hop, commonly referred to as fan-out. So, when we say \"sample a node's first-hop neighborhood with fan-out of 5,\" it means we select a total of 5 neighbors from the node's first hop. In this section, we use DGL's in-built neighbor sampler for obtaining batches of node data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "umFWXCrbuvKS"
   },
   "outputs": [],
   "source": [
    "def create_data_sampler(fanout_list):\n",
    "    '''create a DGL data sampler\n",
    "    Args: layers: the number of hops in the neighborhood that we want to sample\n",
    "    Returns: \n",
    "        a DGL data sampler of type NeighborSampler. \n",
    "        This sampler will sample neighborhood as specified by the fanout_list.\n",
    "        read more about this sampler in the docs: \n",
    "        https://docs.dgl.ai/generated/dgl.dataloading.NeighborSampler.html\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "def create_data_loaders(graph: dgl.DGLGraph, sampler, batch_size: int, train_ids: torch.Tensor, val_ids: torch.Tensor):\n",
    "    '''Given a DGL graph, a sampler, a batch size, and a train/val ratio, \n",
    "    split the graph into training, validation, and test sets\n",
    "    Use the DGL data loader to create data loaders for the training and validation sets\n",
    "    reference: https://docs.dgl.ai/generated/dgl.dataloading.DataLoader.html#dgl.dataloading.DataLoader\n",
    "    Args:\n",
    "        graph: a DGL graph\n",
    "        sampler: a DGL data sampler\n",
    "        batch_size: the size of the batch \n",
    "        train_ratio: the ratio of the training set \n",
    "        val_ratio: the ratio of the validation set\n",
    "    Returns: \n",
    "        train and validation data loader objects\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n0gLK-m0Jx8D"
   },
   "outputs": [],
   "source": [
    "#driver code for sections 1 and 2\n",
    "'''\n",
    "DO NOT CHANGE ANYTHING IN THE CODE BELOW, RUN IT TO TEST YOUR CODE CORRECTNESS\n",
    "Just make sure that you've set the data_path correctly\n",
    "'''\n",
    "\n",
    "src_edges, dst_edges = load_edges_from_csv(f'C:/Users/nimes/Downloads/Fraud_Detection_using_GNN/edge_data.csv')\n",
    "graph = create_graph_from_tensors(src_edges, dst_edges)\n",
    "num_nodes = get_num_nodes(graph)\n",
    "print('Number of nodes in the graph: ', num_nodes)\n",
    "edge_exists = check_if_edge_exists(graph, 0, 1)\n",
    "print('Does the edge (0,1) exist in the graph? ', edge_exists)\n",
    "first_hop_neighbors = get_first_hop_neighbors(graph, 0)\n",
    "print('First hop neighbors of node 0: ', first_hop_neighbors)\n",
    "second_hop_neighbors = get_second_hop_neighbors(graph, 0)\n",
    "print('Second hop neighbors of node 0: ', second_hop_neighbors)\n",
    "graph_features, labels = load_node_information_from_csv(f'C:/Users/nimes/Downloads/Fraud_Detection_using_GNN/node_information.csv')\n",
    "graph = add_node_features_and_labels(graph, graph_features, labels)\n",
    "print('Graph with node features: ', graph)\n",
    "graph = dgl.add_self_loop(graph) #add self loops to prevent 0 degree nodes (DGL crashes when node-degree=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xCSMush_v9Xw"
   },
   "outputs": [],
   "source": [
    "#driver code for section 3, we create a random list of train and validation ids with a 80:20 split and use these ids to instantiate dataloaders\n",
    "'''\n",
    "DO NOT CHANGE ANYTHING IN THE CODE BELOW, RUN IT TO TEST YOUR CODE CORRECTNESS\n",
    "'''\n",
    "\n",
    "\n",
    "#create train and val masks\n",
    "train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "train_mask[torch.randperm(num_nodes)[:int(0.8*num_nodes)]] = True\n",
    "val_mask = ~train_mask\n",
    "\n",
    "#obtain respective ids\n",
    "train_ids = torch.nonzero(train_mask, as_tuple=True)[0]\n",
    "val_ids = torch.nonzero(val_mask, as_tuple=True)[0]\n",
    "\n",
    "#create sampler and data loaders\n",
    "sampler = create_data_sampler([15,15])\n",
    "train_loader, val_loader = create_data_loaders(graph, sampler, 100, train_ids, val_ids)\n",
    "\n",
    "for input_nodes, output_nodes, blocks in train_loader:\n",
    "    print(\"Input nodes in the MFG (Message Flow Graph)\")\n",
    "    print(input_nodes)\n",
    "    print(\"Output nodes in the MFG (Message Flow Graph)\")\n",
    "    print(output_nodes)\n",
    "    print(\"Message Flow Graph used for training\")\n",
    "    print(\"Layer 1\")\n",
    "    print(blocks[0])\n",
    "    print(\"Layer 2\")\n",
    "    print(blocks[1])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CsKeRNnoybLf"
   },
   "source": [
    "<h2>END OF WEEK 1!</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P8u91JzIyrxc"
   },
   "source": [
    "<h2>Week 2</h2>\n",
    "This week, we'll utilize the data we've prepared to construct our very own GCN model, then train and assess it using a validation dataset! Read the resources in the reading material to understand more about GCNSs. The directions stay the same: complete all the unimplemented functions and watch your model come to life!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rfRUf6LyKEY4"
   },
   "outputs": [],
   "source": [
    "#section 4 (Model Building)\n",
    "'''\n",
    "create your first dgl gcn model with 2 hidden layers\n",
    "Remember that 2 layer gcn means that we're \n",
    "looking at the 1st hop and 2nd hop neighbors of the nodes in the batch\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        '''\n",
    "        define the first and second layer of the gcn model using dgl's GraphConv module\n",
    "        read more here: https://docs.dgl.ai/generated/dgl.nn.pytorch.conv.GraphConv.html\n",
    "        make sure to use the correct in_feats and out_feats for the layers\n",
    "        '''\n",
    "\n",
    "        self.conv1 = None #replace None with the definition of the first layer of the gcn model\n",
    "        self.conv2 = None  #replace None with the definition of the first layer of the gcn model\n",
    "        \n",
    "    def forward(self, block, inputs):\n",
    "        '''\n",
    "        Implement the forward pass of the gcn model based on the layers defined in the __init__ function\n",
    "        '''\n",
    "        #remember that you need to pass respective layer information i.e., block[0] for layer 1 and block[1] for layer 2\n",
    "        h = None #replace None with the first layer hidden representation of the nodes in the batch\n",
    "        h = F.relu(h)\n",
    "        h = None #replace None with the second layer hidden representation of the nodes in the batch\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wc7gqQv4yvPm"
   },
   "outputs": [],
   "source": [
    "#section 5 (write evaluate function, refer to the driver code below for hints)\n",
    "\n",
    "def evaluate(model, val_loader, criterion):\n",
    "    '''\n",
    "    Implement the evaluation function and return the loss and accuracy. \n",
    "    The code should be very similar to the train function below, except that you need to compute metrics and not backprop loss\n",
    "\n",
    "    Args:\n",
    "        model: GCN Model\n",
    "        val_loader: validation dataset loader\n",
    "        criterion: loss criterion \n",
    "    Returns: \n",
    "        values of loss and accuracy\n",
    "    '''\n",
    "    #YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bKSh3ZdNy983"
   },
   "outputs": [],
   "source": [
    "#driver code\n",
    "#DO NOT CHANGE ANYTHING IN THE CODE BELOW, RUN IT TO TEST YOUR CODE CORRECTNESS\n",
    "\n",
    "\n",
    "#train function, use this as a helper to complete the evaluate function above\n",
    "def train(model, train_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    for input_nodes, output_nodes, blocks in train_loader:\n",
    "        inputs = blocks[0].srcdata['features'].float()\n",
    "        labels = blocks[1].dstdata['labels']\n",
    "        logits = model(blocks, inputs)\n",
    "        loss = loss_func(logits, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "#initialize the model, optimizer, and criterion\n",
    "in_feat_shape = graph.ndata['features'].shape[1]\n",
    "hidden_size = 16\n",
    "num_classes = 2\n",
    "model = GCN(in_feat_shape, hidden_size, num_classes)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "#train the model for 50 epochs and evaluate every 5 epochs\n",
    "for epoch in range(50):\n",
    "    print(f'Running Epoch {epoch}')\n",
    "    train(model, train_loader, optimizer, loss_func)\n",
    "    if epoch % 5 == 0:\n",
    "      loss, acc = evaluate(model, val_loader, loss_func)\n",
    "      print('Epoch: {}, Loss: {:.4f}, Accuracy: {:.4f}'.format(epoch, loss, acc))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
